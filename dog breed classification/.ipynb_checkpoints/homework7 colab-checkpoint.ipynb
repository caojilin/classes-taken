{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSQjRH3CG5sa"
   },
   "source": [
    "#  Homework 7 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY, teammates A,B,C** (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/)**\n",
    "\n",
    "Handout 4/2/2019, due 4/9/2019 by 4pm.\n",
    "\n",
    "This homework deals with fine-tuning for computer vision. In this task, we attempt to identify 120 different breeds of dogs. The data set used in this competition is actually a subset of the ImageNet data set. Different from the images in the CIFAR-10 data set used in the previous homework, the images in the ImageNet data set are higher and wider and their dimensions are inconsistent. Again, you need to use GPU.\n",
    "\n",
    "The dataset is available at [Kaggle](https://www.kaggle.com/c/dog-breed-identification). The rule is similar to homework 6: \n",
    "\n",
    "- work as a team\n",
    "- submit your results into Kaggle\n",
    "- take a screen shot of your best score and insert it below\n",
    "- the top 3 teams/individuals will be awarded with 500 dollar AWS credits\n",
    "![](ScreenShot.png)\n",
    "First, import the packages or modules required for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1488
    },
    "colab_type": "code",
    "id": "s0NS9Y8FG7mg",
    "outputId": "0f5ddb55-55fb-4b53-cd76-17215686acd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading https://files.pythonhosted.org/packages/40/2b/618811a6331dc0cbb5d9731959f0c2b1b63bc1297c24401a3d7076e05624/d2l-0.8.7.tar.gz\n",
      "Collecting mxnet-cu100\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/36/40b6d201b46495513f7a7fa25fe8b7d85b3602a22efba119e8146d5f1601/mxnet_cu100-1.4.0.post0-py2.py3-none-manylinux1_x86_64.whl (487.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 487.9MB 28kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l) (1.14.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l) (3.0.3)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l) (1.0.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Collecting requests>=2.20.0 (from mxnet-cu100)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 19.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.5.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.4.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (7.4.2)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.2.2)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.4.3)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (6.0.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.6.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->d2l) (40.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->d2l) (1.11.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.10)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.5.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (3.1.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.3.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (3.4.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (5.5.0)\n",
      "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (4.5.3)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (0.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (5.2.4)\n",
      "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->d2l)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
      "\u001b[K    100% |████████████████████████████████| 337kB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->jupyter->d2l) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (2.6.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (0.7.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (4.6.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->d2l) (0.6.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->d2l) (17.0.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->d2l) (0.1.7)\n",
      "Building wheels for collected packages: d2l\n",
      "  Building wheel for d2l (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c7/87/29/22170afbd70e10df77be0339d4e5863f452faa4a2f37ed979f\n",
      "Successfully built d2l\n",
      "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: d2l, graphviz, requests, mxnet-cu100, prompt-toolkit\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: prompt-toolkit 1.0.15\n",
      "    Uninstalling prompt-toolkit-1.0.15:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.15\n",
      "Successfully installed d2l-0.8.7 graphviz-0.8.4 mxnet-cu100-1.4.0.post0 prompt-toolkit-2.0.9 requests-2.21.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "prompt_toolkit",
         "requests"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install d2l mxnet-cu100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRbD02EdG5sb"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import model_zoo, nn\n",
    "from mxnet.gluon import data as gdata, loss as gloss, utils as gutils\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "tOqm5Zv_HAn3",
    "outputId": "8f7ef03d-a0dd-43af-cb02-2627e50c185b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aMB1xLu9HAnF",
    "outputId": "66b2587b-d7e5-4eaa-f6c4-b6dc6372eec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357\n"
     ]
    }
   ],
   "source": [
    "!ls kaggle_dog/test/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jY_-VpbnUrOM",
    "outputId": "6389f590-c8c8-4939-ba5a-5c2f51fe2c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222\n"
     ]
    }
   ],
   "source": [
    "!ls kaggle_dog/train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lEKBTsVFUrRN",
    "outputId": "debddde9-b653-4cd3-a42a-b1766e195bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357\n"
     ]
    }
   ],
   "source": [
    "!ls kaggle_dog/train_valid_test/test/unknown/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAntdKVaG5sf"
   },
   "source": [
    "## Obtain and Organize the Data Sets\n",
    "\n",
    "The competition data is divided into a training set and testing set. The training set contains 10,222 images and the testing set contains 10,357 images. The images in both sets are in JPEG format. These images contain three RGB channels (color) and they have different heights and widths. There are 120 breeds of dogs in the training set, including Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers.\n",
    "\n",
    "### Download the Data Set\n",
    "\n",
    "After logging in to Kaggle, we can click on the \"Data\" tab on the dog breed identification competition webpage shown in Figure 9.17 and download the training data set \"train.zip\", the testing data set \"test.zip\", and the training data set labels \"label.csv.zip\". After downloading the files, place them in the three paths below:\n",
    "\n",
    "* kaggle_dog/train.zip\n",
    "* kaggle_dog/test.zip\n",
    "* kaggle_dog/labels.csv.zip\n",
    "\n",
    "\n",
    "To make it easier to get started, we provide a small-scale sample of the data set mentioned above, \"train_valid_test_tiny.zip\". If you are going to use the full data set for the Kaggle competition, you will also need to change the `demo` variable below to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvxlQDp1IBLp"
   },
   "outputs": [],
   "source": [
    "!mkdir kaggle_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi38Z1i2Hzbl"
   },
   "outputs": [],
   "source": [
    "!cp ../gdrive/My\\ Drive/labels.csv.zip ./kaggle_dog\n",
    "!cp ../gdrive/My\\ Drive/test.zip ./kaggle_dog\n",
    "!cp ../gdrive/My\\ Drive/train.zip ./kaggle_dog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aab223Jwu56a"
   },
   "outputs": [],
   "source": [
    "!cp ../gdrive/My\\ Drive/stanford_lables.csv ./kaggle_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "colab": {},
    "colab_type": "code",
    "id": "op2m7XHKG5sg"
   },
   "outputs": [],
   "source": [
    "# If you use the full data set downloaded for the Kaggle competition, \n",
    "# change the variable below to False.\n",
    "demo = False\n",
    "data_dir = './kaggle_dog'\n",
    "if demo:\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    gutils.download('https://github.com/d2l-ai/d2l-en/raw/master/data/kaggle_dog/train_valid_test_tiny.zip', \n",
    "                    data_dir)\n",
    "    zipfiles = ['train_valid_test_tiny.zip']\n",
    "else:\n",
    "    zipfiles = ['train.zip', 'test.zip', 'labels.csv.zip']\n",
    "for f in zipfiles:\n",
    "    with zipfile.ZipFile(data_dir + '/' + f, 'r') as z:\n",
    "        z.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMBxQPYDG5si"
   },
   "source": [
    "### Organize the Data Set\n",
    "\n",
    "Next, we define the `reorg_train_valid` function to segment the validation set from the original Kaggle competition training set.  The parameter `valid_ratio` in this function is the ratio of the number of examples of each dog breed in the validation set to the number of examples of the breed with the least examples (66) in the original training set. After organizing the data, images of the same breed will be placed in the same folder so that we can read them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0k_OwUCdG5sj"
   },
   "outputs": [],
   "source": [
    "def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n",
    "    # The number of examples of the least represented breed in the training set.\n",
    "    min_n_train_per_label = (\n",
    "        collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # The number of examples of each breed in the validation set.\n",
    "    n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        d2l.mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            d2l.mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            d2l.mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icbQsb59G5sm"
   },
   "source": [
    "The `reorg_dog_data` function below is used to read the training data labels, segment the validation set, and organize the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MKhUn1oqG5sn"
   },
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio):\n",
    "    # Read the training data labels.\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # Skip the file header line (column name).\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n",
    "    # Organize the training set.\n",
    "    d2l.mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQ8WNpF1G5sr"
   },
   "source": [
    "Because we are using a small data set, we set the batch size to 1. During actual training and testing, we would use the entire Kaggle Competition data set and call the `reorg_dog_data` function to organize the data set. Likewise, we would need to set the `batch_size` to a larger integer, such as 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Bf7S1ntEG5ss"
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    # Note: Here, we use a small data set and the batch size should be set\n",
    "    # smaller. When using the complete data set for the Kaggle competition, \n",
    "    # we can set the batch size to a larger integer.\n",
    "    input_dir, batch_size = 'train_valid_test_tiny', 1\n",
    "else:\n",
    "    label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n",
    "    input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
    "    reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMSrFevYG5sw"
   },
   "source": [
    "## Image Augmentation\n",
    "\n",
    "The size of the images in this section are larger than the images in the previous section. Here are some more image augmentation operations that might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CDAlAnK0G5sx"
   },
   "outputs": [],
   "source": [
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    # Randomly crop the image to obtain an image with an area of 0.08 to 1 \n",
    "    # of the original area and height to width ratio between 3/4 and 4/3.\n",
    "    # Then, scale the image to create a new image with a height and width \n",
    "    # of 224 pixels each.\n",
    "    gdata.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                              ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    # Randomly change the brightness, contrast, and saturation.\n",
    "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                              saturation=0.4),\n",
    "    # Add random noise.\n",
    "    gdata.vision.transforms.RandomLighting(0.1),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    # Standardize each channel of the image.\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcKz7L6nG5s2"
   },
   "source": [
    "During testing, we only use definite image preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6sxcXLrPG5s3"
   },
   "outputs": [],
   "source": [
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(256),\n",
    "    # Crop a square of 224 by 224 from the center of the image.\n",
    "    gdata.vision.transforms.CenterCrop(224),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iF6JrlabG5s6"
   },
   "source": [
    "## Read the Data Set\n",
    "\n",
    "As in the previous section, we can create an `ImageFolderDataset` instance to read the data set containing the original image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K8KWhQJ4G5s7"
   },
   "outputs": [],
   "source": [
    "train_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'valid'), flag=1)\n",
    "train_valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train_valid'), flag=1)\n",
    "test_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'test'), flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwFqQOAzG5s_"
   },
   "source": [
    "Here, we create a `DataLoader` instance, just like in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ej1nqCWbG5tA"
   },
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "valid_iter = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_iter = gdata.DataLoader(train_valid_ds.transform_first(\n",
    "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
    "test_iter = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
    "                             batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7_RGEHfG5tC"
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "The data set for this competition is a subset of the ImageNet data set. Therefore, we can use the approach discussed in the [\"Fine Tuning\"](fine-tuning.md) section to select a model pre-trained on the entire ImageNet data set and use it to extract image features to be input in the custom small-scale output network. Gluon provides a wide range of pre-trained models. Here, we will use the pre-trained ResNet-34 model. Because the competition data set is a subset of the pre-training data set, we simply reuse the input of the pre-trained model's output layer, i.e. the extracted features. Then, we can replace the original output layer with a small custom output network that can be trained, such as two fully connected layers in a series. Different from the experiment in the [\"Fine Tuning\"](fine-tuning.md) section, here, we do not retrain the pre-trained model used for feature extraction. This reduces the training time and the memory required to store model parameter gradients.\n",
    "\n",
    "You must note that, during image augmentation, we use the mean values and standard deviations of the three RGB channels for the entire ImageNet data set for normalization. This is consistent with the normalization of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvShN_0SJxAj"
   },
   "outputs": [],
   "source": [
    "finetune_net = model_zoo.vision.resnet152_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TNtJtEBKG5tD"
   },
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    finetune_net = model_zoo.vision.resnet152_v2(pretrained=True)\n",
    "    # Define a new output network.\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # There are 120 output categories.\n",
    "    finetune_net.output_new.add(nn.Dropout(0.5))\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # Initialize the output network.\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # Distribute the model parameters to the CPUs or GPUs used for computation.\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxYz-VzMG5tG"
   },
   "source": [
    "When calculating the loss, we first use the member variable `features` to obtain the input of the pre-trained model's output layer, i.e. the extracted feature. Then, we use this feature as the input for our small custom output network and compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofsHs8mjG5tI"
   },
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def evaluate_loss(data_iter, net, ctx):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.as_in_context(ctx)\n",
    "        output_features = net.features(X.as_in_context(ctx))\n",
    "        outputs = net.output_new(output_features)\n",
    "        l_sum += loss(outputs, y).sum().asscalar()\n",
    "        n += y.size\n",
    "    return l_sum / n\n",
    " \n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbIMUk6wG5tK"
   },
   "source": [
    "## Define the Training Functions\n",
    "\n",
    "We will select the model and tune hyper-parameters according to the model's performance on the validation set. The model training function `train` only trains the small custom output network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxLnton_HkSF"
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    # Only train the small custom output network.\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_iter:\n",
    "            y = y.as_in_context(ctx)\n",
    "            output_features = net.features(X.as_in_context(ctx))\n",
    "            with autograd.record():\n",
    "                outputs = net.output_new(output_features)\n",
    "                l = loss(outputs, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc += accuracy(outputs, y)\n",
    "            n += y.size\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, ctx)\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, valid_loss))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n))\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))\n",
    "        print(\"acc\",100*train_acc/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBgjGIexG5tN"
   },
   "source": [
    "## Train and Validate the Model\n",
    "\n",
    "Now, we can train and validate the model. The following hyper-parameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 10 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "jZUhY_ZNG5tO",
    "outputId": "78a361ad-9613-47d4-e760-564871102be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 4.749614, valid loss 4.410706, time 227.03 sec, lr 0.001\n",
      "acc 0.0002353671158778996\n",
      "epoch 2, train loss 4.331453, valid loss 3.954857, time 227.33 sec, lr 0.001\n",
      "acc 0.0008082728204890769\n",
      "epoch 3, train loss 3.924156, valid loss 3.428005, time 226.80 sec, lr 0.001\n",
      "acc 0.001568255368545667\n",
      "epoch 4, train loss 3.497879, valid loss 2.893896, time 226.77 sec, lr 0.001\n",
      "acc 0.002283729741420778\n",
      "epoch 5, train loss 3.085064, valid loss 2.391269, time 225.31 sec, lr 0.001\n",
      "acc 0.0029495983305942936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f7a5120e17ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybridize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n\u001b[0;32m----> 5\u001b[0;31m       lr_decay)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-c4e6ff8b5b35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period, lr_decay)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 30, 0.001, 1e-4\n",
    "lr_period, lr_decay, net = 100, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "# train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "#       lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNN70irZE5Jn"
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "bijxO_9xE8Ei",
    "outputId": "e0e85f2e-e348-4de9-daf6-12f96752dac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.898167, time 238.20 sec, lr 0.0001\n",
      "acc 0.6041461930212455\n",
      "epoch 2, train loss 0.905427, time 237.09 sec, lr 0.0001\n",
      "acc 0.6033555078450492\n",
      "epoch 3, train loss 0.888114, time 236.72 sec, lr 0.0001\n",
      "acc 0.6064654446189311\n",
      "epoch 4, train loss 0.880783, time 236.99 sec, lr 0.0001\n",
      "acc 0.6074465054678277\n",
      "epoch 5, train loss 0.887276, time 236.05 sec, lr 0.0001\n",
      "acc 0.6068587029103872\n",
      "epoch 6, train loss 0.871785, time 235.81 sec, lr 0.0001\n",
      "acc 0.6084762034036483\n",
      "epoch 7, train loss 0.877785, time 235.40 sec, lr 0.0001\n",
      "acc 0.6088180461290927\n",
      "epoch 8, train loss 0.885707, time 234.43 sec, lr 0.0001\n",
      "acc 0.606615521557479\n",
      "epoch 9, train loss 0.872738, time 233.36 sec, lr 0.0001\n",
      "acc 0.6055608114788523\n",
      "epoch 10, train loss 0.884618, time 233.79 sec, lr 0.0001\n",
      "acc 0.6047951390285637\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 10, 0.0001, 1e-4\n",
    "\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "dKjpS3Zzk817",
    "outputId": "6c73dae1-57b0-481e-d5ca-5b8ac317cdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.876388, time 237.04 sec, lr 0.0001\n",
      "acc 0.6053176307090457\n",
      "epoch 2, train loss 0.906797, time 237.36 sec, lr 0.0001\n",
      "acc 0.6034708447569248\n",
      "epoch 3, train loss 0.855348, time 235.24 sec, lr 0.0001\n",
      "acc 0.6124907727345075\n",
      "epoch 4, train loss 0.863292, time 236.68 sec, lr 0.0001\n",
      "acc 0.611229011443841\n",
      "epoch 5, train loss 0.862772, time 236.06 sec, lr 0.0001\n",
      "acc 0.6108718827101518\n",
      "epoch 6, train loss 0.872391, time 235.23 sec, lr 0.0001\n",
      "acc 0.6090376037040864\n",
      "epoch 7, train loss 0.871941, time 236.75 sec, lr 0.0001\n",
      "acc 0.6083511386079066\n",
      "epoch 8, train loss 0.888528, time 238.29 sec, lr 0.0001\n",
      "acc 0.604283764179941\n",
      "epoch 9, train loss 0.862216, time 238.64 sec, lr 0.0001\n",
      "acc 0.609101525633052\n",
      "epoch 10, train loss 0.845928, time 238.00 sec, lr 0.0001\n",
      "acc 0.6135093538384688\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 10, 0.0001, 1e-4\n",
    "\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "gNA57HNKvqYW",
    "outputId": "b8fde2b1-99e9-4ec0-e0fb-7618ae03cc0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.864161, time 237.20 sec, lr 1e-05\n",
      "acc 0.6137511450771809\n",
      "epoch 2, train loss 0.889694, time 237.15 sec, lr 1e-05\n",
      "acc 0.6074478955820241\n",
      "epoch 3, train loss 0.861888, time 235.03 sec, lr 1e-05\n",
      "acc 0.6107954544182324\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 3, 0.00001, 1e-4\n",
    "\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "C8hae8cfz-Wq",
    "outputId": "d2c35d9d-df3d-4a8c-97e9-9c5f4038427f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.855360, time 237.01 sec, lr 1e-05\n",
      "acc 0.6118890736999169\n",
      "epoch 2, train loss 0.849515, time 236.10 sec, lr 1e-05\n",
      "acc 0.6117000875583112\n",
      "epoch 3, train loss 0.859526, time 235.40 sec, lr 1e-05\n",
      "acc 0.6098519126581973\n",
      "epoch 4, train loss 0.866032, time 236.02 sec, lr 1e-05\n",
      "acc 0.605558032416663\n",
      "epoch 5, train loss 0.852985, time 235.11 sec, lr 1e-05\n",
      "acc 0.6132675625997568\n",
      "epoch 6, train loss 0.873989, time 235.40 sec, lr 1e-05\n",
      "acc 0.6037251429416923\n",
      "epoch 7, train loss 0.869704, time 234.71 sec, lr 1e-05\n",
      "acc 0.6052147995770223\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 7, 0.00001, 1e-4\n",
    "\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itVlOxf_G5tT"
   },
   "source": [
    "## Classify the Testing Set and Submit Results on Kaggle\n",
    "\n",
    "After obtaining a satisfactory model design and hyper-parameters, we use all training data sets (including validation sets) to retrain the model and then classify the testing set. Note that predictions are made by the output network we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "colab": {},
    "colab_type": "code",
    "id": "66sbqrRlG5tU"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output_features = net.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net.output_new(output_features))\n",
    "    preds.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9wDakmRXEEr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kck1ds3dT9Qp"
   },
   "outputs": [],
   "source": [
    "file_name = \"net.params\"\n",
    "net.save_parameters(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKe-uILIUBGR"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cz6J4eDVj23"
   },
   "outputs": [],
   "source": [
    "# files.download('submission.csv')\n",
    "# files.download('net.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "BZDCLFkKUV3F",
    "outputId": "7710b1eb-0790-4b27-d2c4-68446148e7ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 16.3M/16.3M [00:08<00:00, 2.09MB/s]\n",
      "Successfully submitted to Dog Breed Identification"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c dog-breed-identification -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3h8-KKl0UYuZ"
   },
   "outputs": [],
   "source": [
    "!cp submission.csv ../gdrive/My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUp5S96xW1aa"
   },
   "outputs": [],
   "source": [
    "USER_ID = 'caojilin' # REPLACE WITH YOUR OWN USER NAME\n",
    "USER_SECRET = \"95413dff633e902356ea9fc5b87e682d\" # REPLACE WITH YOUR OWN PRIVATE API TOKEN\n",
    "import os, json, nbformat, pandas as pd\n",
    "KAGGLE_CONFIG_DIR = os.path.join(os.path.expandvars('$HOME'), '.kaggle')\n",
    "os.makedirs(KAGGLE_CONFIG_DIR, exist_ok = True)\n",
    "with open(os.path.join(KAGGLE_CONFIG_DIR, 'kaggle.json'), 'w') as f:\n",
    "    json.dump({'username': USER_ID, 'key': USER_SECRET}, f)\n",
    "!chmod 600 {KAGGLE_CONFIG_DIR}/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YILTlXkWG5ta"
   },
   "source": [
    "After executing the above code, we will generate a \"submission.csv\" file. The format of this file is consistent with the Kaggle competition requirements. \n",
    "\n",
    "## Hints to Improve Your Results\n",
    "\n",
    "* You should download the whole data set from Kaggle and switch to `demo=False`. \n",
    "* Try to increase the `batch_size` (batch size) and `num_epochs` (number of epochs).\n",
    "* Try a deeper pre-trained model, you may find models from [gluoncv](https://gluon-cv.mxnet.io/model_zoo/classification.html)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework7.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
